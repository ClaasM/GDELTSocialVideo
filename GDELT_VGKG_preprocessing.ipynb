{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the VGKG Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the relevant data\n",
    "\n",
    "- Download the HTML from the DocumentIdentifier and extract/save its tokens\n",
    "- Download the Images and classify them with tinyYoloV3 (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib\n",
    "import time\n",
    "from gzip import GzipFile\n",
    "import pandas as pd\n",
    "import os\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import pickle\n",
    "import threading\n",
    "from multiprocessing.dummy import Pool # use threads for I/O bound tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use crawler.py instead of the following cell - it uses multiprocessing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_count = 5000\n",
    "current_time = time.time()\n",
    "article_path = \"data/GDELT_VGKG/preprocessed/articles/%d/\" % current_time\n",
    "image_path = \"data/GDELT_VGKG/preprocessed/images/%d/\" % current_time\n",
    "\n",
    "print(image_path)\n",
    "print(article_path)\n",
    "\n",
    "with GzipFile('data/GDELT_VGKG/vgkg-20160427-part1.csv.gz') as gzipfile:\n",
    "    df = pd.read_csv(gzipfile, nrows=data_count)\n",
    "    \n",
    "    os.makedirs(article_path)\n",
    "    os.makedirs(image_path)\n",
    "    \n",
    "    f = FloatProgress(min=0, max=data_count)\n",
    "    display(f)\n",
    "    for index, article in df.iterrows():\n",
    "        f.value += 1\n",
    "        try:\n",
    "            doc = urllib.request.urlopen(article.DocumentIdentifier).read()\n",
    "            bs_doc = BeautifulSoup(doc)\n",
    "\n",
    "            # remove some tags that aren't rendered, including their content\n",
    "            # From https://www.w3schools.com/tags/ref_byfunc.asp\n",
    "            programming_tags = ['script', 'noscript', 'applet', 'embed', 'object', 'param']\n",
    "            meta_tags = ['head', 'meta', 'base', 'basefont']\n",
    "            other_tags = ['data', 'style']\n",
    "            [x.extract() for x in bs_doc.findAll(programming_tags + meta_tags + other_tags)]\n",
    "\n",
    "            # Keep only the remaining text (removing all tags etc.)\n",
    "            text = bs_doc.get_text() #re.sub('<[^<]+?>', '', str(doc))[:100]\n",
    "\n",
    "            # Tokenize the text\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "\n",
    "            # Keep only tokens that are words and more than a letter\n",
    "            alpha_tokens = [token for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "            # Keep only tokens that are either all caps or no caps or start with a capital letter\n",
    "            pattern = re.compile(\"(^[A-Z]?[a-z]+$)|(^[A-Z]+$)\")\n",
    "            word_tokens = [token for token in alpha_tokens if pattern.match(token)]\n",
    "\n",
    "\n",
    "            # Done preprocessing. Save tokens\n",
    "            file = open(\"%s/%s\" % (article_path, article.DATE), \"wb+\")\n",
    "            pickle.dump(word_tokens, file) # Date is unique(?) TODO find out\n",
    "            \n",
    "            # Download the corresponding image \n",
    "            # (for the whole dataset, we'll have to classify and then discard, unless I get proper storage)\n",
    "            urllib.request.urlretrieve(article.ImageURL, \"%s/%s\" % (image_path, article.DATE))\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e, article.DocumentIdentifier)\n",
    "            \n",
    "            \n",
    "            # TODO create number of URLS (including splitting by error type) \n",
    "            # and Number of average Characters at each stage plots\n",
    "            # TODO look into the errors in more detail (especially 403's etc.)\n",
    "            # TODO store and publicise raw html/preprocessed/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5ec7f351cf487bb65b170d3a890041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=41.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{b'person': 5}\n",
      "{}\n",
      "{b'person': 3, b'tie': 1}\n",
      "{b'person': 1}\n",
      "{b'person': 1}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{b'person': 2}\n",
      "{b'person': 2}\n",
      "{b'person': 3, b'chair': 1}\n",
      "{b'person': 3}\n",
      "{b'person': 2}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{b'person': 4}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{}\n",
      "{}\n",
      "{b'car': 3}\n",
      "{b'person': 2}\n",
      "{b'person': 1}\n",
      "{b'person': 2}\n",
      "{b'person': 1, b'bicycle': 1}\n",
      "{}\n",
      "{b'person': 3}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{b'person': 1}\n",
      "{b'tie': 1}\n",
      "{}\n",
      "{b'person': 2}\n",
      "{b'person': 1}\n",
      "CPU times: user 4.44 s, sys: 450 ms, total: 4.89 s\n",
      "Wall time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_time = 1535141748 # set this to which version of the collected data is to be used\n",
    "image_path = \"data/GDELT_VGKG/preprocessed/images/%d/\" % dataset_time\n",
    "image_classification_path = \"data/GDELT_VGKG/preprocessed/image_classifications/yoloV3-tiny/%d/%d/\" % (dataset_time, time.time())\n",
    "# TODO make the testing of different classifiers DRY\n",
    "\n",
    "os.makedirs(image_classification_path)\n",
    "\n",
    "sys.path += [os.getcwd()]\n",
    "os.environ['DYLD_LIBRARY_PATH'] = \"/usr/local/cuda/lib\"\n",
    "%run darknet_wrapper.py\n",
    "\n",
    "MODEL=\"yolov3-tiny\"\n",
    "net, meta = initialize_classifier(config=\"cfg/%s.cfg\"%MODEL, weights=\"weights/%s.weights\"%MODEL, data=\"cfg/coco.data\")\n",
    "\n",
    "images = os.listdir(image_path)\n",
    "f = FloatProgress(min=0, max=len(images))\n",
    "display(f)\n",
    "for image in images:\n",
    "    f.value += 1\n",
    "    try:\n",
    "        image_dir = image_path + \"/\" + image\n",
    "        result = detect(net, meta, image_dir)\n",
    "        labels = dict()\n",
    "        for label, probability, coordinates in result:\n",
    "            if label in labels: \n",
    "                labels[label]+=1 \n",
    "            else:\n",
    "                labels[label] = 1\n",
    "        print(labels)\n",
    "        # Save classification result\n",
    "        file = open(\"%s/%s\" % (image_classification_path, image), \"wb+\")\n",
    "        pickle.dump(labels, file)\n",
    "        \n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            # Some of the files are not actually images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
