{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the VGKG Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the relevant data\n",
    "\n",
    "- Download the HTML from the DocumentIdentifier and extract/save its tokens\n",
    "- Download the Images and classify them with tinyYoloV3 (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claasmeiners/.virtualenvs/Thesis/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/claasmeiners/.virtualenvs/Thesis/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import urllib\n",
    "import time\n",
    "from gzip import GzipFile\n",
    "import pandas as pd\n",
    "import os\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397ace2ca0f84b8fa29ab698050ffa75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 410: Gone\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n",
      "<urlopen error [Errno 8] nodename nor servname provided, or not known>\n",
      "HTTP Error 404: Not Found\n",
      "<urlopen error [Errno 8] nodename nor servname provided, or not known>\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 404: Not Found\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:748)>\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n",
      "[Errno 54] Connection reset by peer\n",
      "HTTP Error 410: Gone\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n",
      "<urlopen error [Errno 51] Network is unreachable>\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_count = 100\n",
    "current_time = time.time()\n",
    "article_path = \"data/GDELT_VGKG/preprocessed/articles/%d/\" % current_time\n",
    "image_path = \"data/GDELT_VGKG/preprocessed/images/%d/\" % current_time\n",
    "\n",
    "print(image_path)\n",
    "print(article_path)\n",
    "\n",
    "with GzipFile('data/GDELT_VGKG/vgkg-20160427-part1.csv.gz') as gzipfile:\n",
    "    df = pd.read_csv(gzipfile, nrows=data_count)\n",
    "    \n",
    "    os.makedirs(article_path)\n",
    "    os.makedirs(image_path)\n",
    "    \n",
    "    f = FloatProgress(min=0, max=data_count)\n",
    "    display(f)\n",
    "    for index, article in df.iterrows():\n",
    "        f.value += 1\n",
    "        try:\n",
    "            doc = urllib.request.urlopen(article.DocumentIdentifier).read()\n",
    "            bs_doc = BeautifulSoup(doc)\n",
    "\n",
    "            # remove some tags that aren't rendered, including their content\n",
    "            # From https://www.w3schools.com/tags/ref_byfunc.asp\n",
    "            programming_tags = ['script', 'noscript', 'applet', 'embed', 'object', 'param']\n",
    "            meta_tags = ['head', 'meta', 'base', 'basefont']\n",
    "            other_tags = ['data', 'style']\n",
    "            [x.extract() for x in bs_doc.findAll(programming_tags + meta_tags + other_tags)]\n",
    "\n",
    "            # Keep only the remaining text (removing all tags etc.)\n",
    "            text = bs_doc.get_text() #re.sub('<[^<]+?>', '', str(doc))[:100]\n",
    "\n",
    "            # Tokenize the text\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "\n",
    "            # Keep only tokens that are words and more than a letter\n",
    "            alpha_tokens = [token for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "            # Keep only tokens that are either all caps or no caps or start with a capital letter\n",
    "            pattern = re.compile(\"(^[A-Z]?[a-z]+$)|(^[A-Z]+$)\")\n",
    "            word_tokens = [token for token in alpha_tokens if pattern.match(token)]\n",
    "\n",
    "\n",
    "            # Done preprocessing. Save tokens\n",
    "            file = open(\"%s/%s\" % (article_path, article.DATE), \"wb+\")\n",
    "            pickle.dump(word_tokens, file) # Date is unique(?) TODO find out\n",
    "            \n",
    "            # Download the corresponding image \n",
    "            # (for the whole dataset, we'll have to classify and then discard, unless I get proper storage)\n",
    "            urllib.request.urlretrieve(article.ImageURL, \"%s/%s\" % (image_path, article.DATE))\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "            # TODO create number of URLS (including splitting by error type) \n",
    "            # and Number of average Characters at each stage plots\n",
    "            # TODO look into the errors in more detail (especially 403's etc.)\n",
    "            # TODO store and publicise raw html/preprocessed/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5ec7f351cf487bb65b170d3a890041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=41.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{b'person': 5}\n",
      "{}\n",
      "{b'person': 3, b'tie': 1}\n",
      "{b'person': 1}\n",
      "{b'person': 1}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{b'person': 2}\n",
      "{b'person': 2}\n",
      "{b'person': 3, b'chair': 1}\n",
      "{b'person': 3}\n",
      "{b'person': 2}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{b'person': 4}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{}\n",
      "{}\n",
      "{b'car': 3}\n",
      "{b'person': 2}\n",
      "{b'person': 1}\n",
      "{b'person': 2}\n",
      "{b'person': 1, b'bicycle': 1}\n",
      "{}\n",
      "{b'person': 3}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{b'person': 1}\n",
      "{b'person': 1}\n",
      "{b'tie': 1}\n",
      "{}\n",
      "{b'person': 2}\n",
      "{b'person': 1}\n",
      "CPU times: user 4.44 s, sys: 450 ms, total: 4.89 s\n",
      "Wall time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_time = 1535141748 # set this to which version of the collected data is to be used\n",
    "image_path = \"data/GDELT_VGKG/preprocessed/images/%d/\" % dataset_time\n",
    "image_classification_path = \"data/GDELT_VGKG/preprocessed/image_classifications/%d/%d/\" % (dataset_time, time.time())\n",
    "\n",
    "os.makedirs(image_classification_path)\n",
    "\n",
    "sys.path += [os.getcwd()]\n",
    "os.environ['DYLD_LIBRARY_PATH'] = \"/usr/local/cuda/lib\"\n",
    "%run darknet_wrapper.py\n",
    "\n",
    "MODEL=\"yolov3-tiny\"\n",
    "net, meta = initialize_classifier(config=\"cfg/%s.cfg\"%MODEL, weights=\"weights/%s.weights\"%MODEL, data=\"cfg/coco.data\")\n",
    "\n",
    "images = os.listdir(image_path)\n",
    "f = FloatProgress(min=0, max=len(images))\n",
    "display(f)\n",
    "for image in images:\n",
    "    f.value += 1\n",
    "    try:\n",
    "        image_dir = image_path + \"/\" + image\n",
    "        result = detect(net, meta, image_dir)\n",
    "        labels = dict()\n",
    "        for label, probability, coordinates in result:\n",
    "            if label in labels: \n",
    "                labels[label]+=1 \n",
    "            else:\n",
    "                labels[label] = 1\n",
    "        print(labels)\n",
    "        # Save classification result\n",
    "        file = open(\"%s/%s\" % (image_classification_path, image), \"wb+\")\n",
    "        pickle.dump(labels, file)\n",
    "        \n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            # Some of the files are not actually images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
